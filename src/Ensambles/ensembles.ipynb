{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PjQglGj4q54"
      },
      "source": [
        "# Случайные леса\n",
        "\n",
        "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с бустингом, предоставляемым библиотекой `CatBoost`.\n",
        "\n",
        "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LH5PiGz04q5-"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from itertools import product\n",
        "from typing import Callable, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import pandas\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9sAjVNUICu76",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "# Этой функцией будут помечены все места, которые необходимо дозаполнить\n",
        "# Это могут быть как целые функции, так и отдельные части внутри них\n",
        "# Всегда можно воспользоваться интроспекцией и найти места использования этой функции :)\n",
        "def todo():\n",
        "    stack = inspect.stack()\n",
        "    caller_frame = stack[1]\n",
        "    function_name = caller_frame.function\n",
        "    line_number = caller_frame.lineno\n",
        "    raise NotImplementedError(f\"TODO at {function_name}, line {line_number}\")\n",
        "\n",
        "\n",
        "SEED = 0xC0FFEE\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Me8wXP7fCu77"
      },
      "outputs": [],
      "source": [
        "def mode(data):\n",
        "    counts = Counter(data)\n",
        "    return counts.most_common(n=1)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfxycK0Q4q5_"
      },
      "source": [
        "### Задание 1 (2 балла)\n",
        "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bootstrap. Элементы, которые не вошли в новую обучающую выборку, образуют **out-of-bag** выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
        "\n",
        "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
        "\n",
        "#### Методы\n",
        "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
        "\n",
        "#### Параметры конструктора\n",
        "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bootstrap. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
        "\n",
        "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
        "\n",
        "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
        "\n",
        "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
        "\n",
        "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bQUJHTjS4q5-"
      },
      "outputs": [],
      "source": [
        "# Для начала реализуем сами критерии\n",
        "\n",
        "\n",
        "def gini(x: npt.ArrayLike) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the Gini impurity of a list or array of class labels.\n",
        "\n",
        "    Args:\n",
        "        x (ArrayLike): Array-like object containing class labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Gini impurity value.\n",
        "    \"\"\"\n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    counter = Counter(x)\n",
        "    total = len(x)\n",
        "    sum_sq_probs = sum((count / total) ** 2 for count in counter.values())\n",
        "    \n",
        "    return 1.0 - sum_sq_probs\n",
        "\n",
        "\n",
        "\n",
        "def entropy(x: npt.ArrayLike) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a list or array of class labels.\n",
        "\n",
        "    Args:\n",
        "        x (ArrayLike): Array-like object containing class labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Entropy value.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    counter = Counter(x)\n",
        "    total = len(x)\n",
        "    \n",
        "    entropy = 0.0\n",
        "    for count in counter.values():\n",
        "        probability = count / total\n",
        "        entropy -= probability * np.log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "\n",
        "def gain(left_y: npt.ArrayLike, right_y: npt.ArrayLike, criterion: Callable[[npt.ArrayLike], float]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a split using a specified criterion.\n",
        "\n",
        "    Args:\n",
        "        left_y (ArrayLike): Class labels for the left split.\n",
        "        right_y (ArrayLike): Class labels for the right split.\n",
        "        criterion (Callable): Function to calculate impurity (e.g., gini or entropy).\n",
        "\n",
        "    Returns:\n",
        "        float: Information gain from the split.\n",
        "    \"\"\"\n",
        "    left_y, right_y = np.asarray(left_y), np.asarray(right_y)\n",
        "    y = np.concatenate([left_y, right_y])\n",
        "    R_l, R_r = len(left_y), len(right_y)\n",
        "    return criterion(y) - (R_l / len(y)) * criterion(left_y) - (R_r / len(y)) * criterion(right_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-aePZqt3Cu78",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DecisionTreeLeaf:\n",
        "    classes: np.ndarray\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.max_class = mode(self.classes)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecisionTreeInternalNode:\n",
        "    split_dim: int\n",
        "    left: Union[\"DecisionTreeInternalNode\", DecisionTreeLeaf]\n",
        "    right: Union[\"DecisionTreeInternalNode\", DecisionTreeLeaf]\n",
        "\n",
        "\n",
        "DecisionTreeNode = Union[DecisionTreeInternalNode, DecisionTreeLeaf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8smLW2V_4q5_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
        "        self.X = np.asarray(X)\n",
        "        self.y = np.asarray(y)\n",
        "\n",
        "        bootstrap_data = self._generate_bootstrap_samples()\n",
        "        self._boot_X, self._boot_y, self._out_of_bag_X, self._out_of_bag_y = bootstrap_data\n",
        "        \n",
        "        self._criterion = gini if criterion == \"gini\" else entropy\n",
        "        self._max_depth = max_depth\n",
        "        self._min_samples_leaf = min_samples_leaf\n",
        "        self._max_features = max_features if max_features != \"auto\" else int((X.shape[1])**0.5)\n",
        "        \n",
        "        self._root = self._build_node(self._boot_X, self._boot_y, 0)\n",
        "\n",
        "    @property\n",
        "    def out_of_bag(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        return self._out_of_bag_X, self._out_of_bag_y\n",
        "    \n",
        "    def _generate_bootstrap_samples(self):\n",
        "        n_samples = self.X.shape[0]\n",
        "        bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        bootstrap_X = self.X[bootstrap_indices]\n",
        "        bootstrap_y = self.y[bootstrap_indices]\n",
        "        \n",
        "        all_indices = set(range(n_samples))\n",
        "        bootstrap_unique_indices = set(bootstrap_indices)\n",
        "        oob_indices = list(all_indices - bootstrap_unique_indices)\n",
        "        \n",
        "        oob_X = self.X[oob_indices] if oob_indices else np.array([])\n",
        "        oob_y = self.y[oob_indices] if oob_indices else np.array([])\n",
        "        \n",
        "        return bootstrap_X, bootstrap_y, oob_X, oob_y\n",
        "    \n",
        "\n",
        "    def _build_node(self, points: np.ndarray, classes: np.ndarray, depth: int) -> DecisionTreeNode:\n",
        "        _max_depth = self._max_depth is not None and depth >= self._max_depth\n",
        "        _max_samples = len(classes) < 2 * self._min_samples_leaf\n",
        "        _same_classes = len(np.unique(classes)) == 1\n",
        "\n",
        "        if _max_depth or _max_samples or _same_classes:\n",
        "            return DecisionTreeLeaf(classes)\n",
        "\n",
        "        best_split = self._find_split(points, classes)\n",
        "        \n",
        "        if best_split is None:\n",
        "            return DecisionTreeLeaf(classes)\n",
        "        \n",
        "        split_dim, left_mask, right_mask = best_split\n",
        "        \n",
        "\n",
        "        left_node = self._build_node(points[left_mask], classes[left_mask], depth + 1)\n",
        "        right_node = self._build_node(points[right_mask], classes[right_mask], depth + 1)\n",
        "        \n",
        "        return DecisionTreeInternalNode(\n",
        "            split_dim=split_dim,\n",
        "            left=left_node,\n",
        "            right=right_node\n",
        "        )\n",
        "    \n",
        "    def _find_split(self, points, classes):\n",
        "        _, n_features = points.shape\n",
        "        best_gain = -np.inf\n",
        "        best_split = None\n",
        "\n",
        "        \n",
        "        \n",
        "        trying_features = np.random.choice(\n",
        "            n_features, \n",
        "            size=min(self._max_features, n_features), \n",
        "            replace=False\n",
        "        )\n",
        "        \n",
        "        for feature_idx in trying_features:\n",
        "            feature_values = points[:, feature_idx]\n",
        "            unique_values = np.unique(feature_values)\n",
        "            \n",
        "            if len(unique_values) <= 1:\n",
        "                continue\n",
        "\n",
        "            left_mask = feature_values == 0\n",
        "            right_mask = ~left_mask\n",
        "            \n",
        "            if (np.sum(left_mask) < self._min_samples_leaf or \n",
        "                np.sum(right_mask) < self._min_samples_leaf):\n",
        "                continue\n",
        "            \n",
        "            current_gain = gain(classes[left_mask], classes[right_mask], self._criterion)\n",
        "            \n",
        "            if current_gain > best_gain:\n",
        "                best_gain = current_gain\n",
        "                best_split = (feature_idx, left_mask, right_mask)\n",
        "        \n",
        "        return best_split\n",
        "    \n",
        "    def _predict(self, points: np.ndarray, node: DecisionTreeNode) -> np.ndarray:\n",
        "        if isinstance(node, DecisionTreeLeaf):\n",
        "            return np.full(points.shape[0], node.max_class)\n",
        "        \n",
        "        left_mask = points[:, node.split_dim] == 0\n",
        "        right_mask = ~left_mask\n",
        "        predictions = np.empty(points.shape[0], dtype=self.y.dtype)\n",
        "    \n",
        "        if np.any(left_mask):\n",
        "            predictions[left_mask] = self._predict(points[left_mask], node.left)\n",
        "        \n",
        "        if np.any(right_mask):\n",
        "            predictions[right_mask] = self._predict(points[right_mask], node.right)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "    def predict(self, points: np.ndarray) -> np.ndarray:\n",
        "        return self._predict(points, self._root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oijgwLt4q6A"
      },
      "source": [
        "### Задание 2 (2 балла)\n",
        "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
        "\n",
        "#### Параметры конструктора\n",
        "`n_estimators` - количество используемых для предсказания деревьев.\n",
        "\n",
        "Остальное - параметры деревьев.\n",
        "\n",
        "#### Методы\n",
        "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
        "\n",
        "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "APIy88YW4q6A"
      },
      "outputs": [],
      "source": [
        "class RandomForestClassifier:\n",
        "\n",
        "    _n_features: int = None\n",
        "\n",
        "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10, random_state=21):\n",
        "        self._criterion = criterion\n",
        "        self._max_depth = max_depth\n",
        "        self._min_samples_leaf = min_samples_leaf\n",
        "        self._max_features = max_features\n",
        "        self._n_estimators = n_estimators\n",
        "        self._estimators = []\n",
        "        self._random_state = random_state\n",
        "\n",
        "    @property\n",
        "    def estimators(self) -> List[DecisionTree]:\n",
        "        return self._estimators\n",
        "\n",
        "    @property\n",
        "    def n_features(self) -> int:\n",
        "        if self._n_features is None:\n",
        "            raise RuntimeError(\"Fit random forest before accessing to number of features properties\")\n",
        "        return self._n_features\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        self._n_features = X.shape[1]\n",
        "        \n",
        "        self._estimators = []\n",
        "        for _ in range(self._n_estimators):\n",
        "            tree = DecisionTree(\n",
        "                X=X,\n",
        "                y=y,\n",
        "                criterion=self._criterion,\n",
        "                max_depth=self._max_depth,\n",
        "                min_samples_leaf=self._min_samples_leaf,\n",
        "                max_features=self._max_features\n",
        "            )\n",
        "            self._estimators.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        if len(self._estimators) == 0:\n",
        "            raise RuntimeError(\"Model not fitted yet\")\n",
        "        \n",
        "        all_predictions = []\n",
        "        for tree in self._estimators:\n",
        "            pred = tree.predict(X)\n",
        "            all_predictions.append(pred)\n",
        "\n",
        "        predictions_matrix = np.column_stack(all_predictions)\n",
        "        final_predictions = []\n",
        "        for i in range(predictions_matrix.shape[0]):\n",
        "            row = predictions_matrix[i, :]\n",
        "            unique, counts = np.unique(row, return_counts=True)\n",
        "            most_common = unique[counts.argmax()]\n",
        "            final_predictions.append(most_common)\n",
        "        \n",
        "        return np.array(final_predictions)\n",
        "    \n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'criterion': self._criterion,\n",
        "            'max_depth': self._max_depth,\n",
        "            'min_samples_leaf': self._min_samples_leaf,\n",
        "            'max_features': self._max_features,\n",
        "            'n_estimators': self._n_estimators,\n",
        "            'random_state': self._random_state\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for key, value in params.items():\n",
        "            if key == 'criterion':\n",
        "                self._criterion = value\n",
        "            elif key == 'max_depth':\n",
        "                self._max_depth = value\n",
        "            elif key == 'min_samples_leaf':\n",
        "                self._min_samples_leaf = value\n",
        "            elif key == 'max_features':\n",
        "                self._max_features = value\n",
        "            elif key == 'n_estimators':\n",
        "                self._n_estimators = value\n",
        "            elif key == 'random_state':\n",
        "                self._random_state = value\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i80pffMn4q6A"
      },
      "source": [
        "### Задание 3 (2 балла)\n",
        "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
        "\n",
        "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rEmVG1Fl4q6B"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = y_true.reshape(-1)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "\n",
        "def feature_importance(rfc: RandomForestClassifier) -> np.ndarray:\n",
        "    n_features = rfc.n_features\n",
        "    importance = np.zeros(n_features)\n",
        "\n",
        "    err_oob_list = []\n",
        "    for tree in rfc.estimators:\n",
        "        oob_X, oob_y = tree.out_of_bag\n",
        "        if len(oob_y) > 0:\n",
        "            pred = tree.predict(oob_X)\n",
        "            err_oob = 1 - accuracy_score(oob_y, pred)\n",
        "            err_oob_list.append(err_oob)\n",
        "        else:\n",
        "            err_oob_list.append(0.0)\n",
        "    \n",
        "    for feature_idx in range(n_features):\n",
        "        feature_importances = []\n",
        "        \n",
        "        for tree, err_oob in zip(rfc.estimators, err_oob_list):\n",
        "            oob_X, oob_y = tree.out_of_bag\n",
        "            \n",
        "            if len(oob_y) == 0:\n",
        "                continue\n",
        "            \n",
        "            shuffled_X = oob_X.copy()\n",
        "            np.random.shuffle(shuffled_X[:, feature_idx])\n",
        "            \n",
        "            pred_shuffled = tree.predict(shuffled_X)\n",
        "            err_oob_j = 1 - accuracy_score(oob_y, pred_shuffled)\n",
        "            \n",
        "            feature_importances.append(err_oob_j - err_oob)\n",
        "        importance[feature_idx] = np.mean(feature_importances) if feature_importances else 0.0\n",
        "    \n",
        "    return importance\n",
        "\n",
        "\n",
        "def most_important_features(importance, names, k=20):\n",
        "    # Выводит названия k самых важных признаков\n",
        "    indices = np.argsort(importance)[::-1][:k]\n",
        "    return np.array(names)[indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JooN_YKm4q6B"
      },
      "source": [
        "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате точность должна быть примерно равна `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8gqYMp994q6B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Importance: [-1.05920915e-03 -6.14926864e-04  1.72753287e-01  1.63075028e-01\n",
            "  3.34761856e-01 -1.30265256e-04]\n"
          ]
        }
      ],
      "source": [
        "def synthetic_dataset(size):\n",
        "    X = [\n",
        "        (np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, i % 6 == 0, i % 3 == 2, np.random.randint(0, 2))\n",
        "        for i in range(size)\n",
        "    ]\n",
        "    y = [i % 3 for i in range(size)]\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "X, y = synthetic_dataset(1000)\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X, y)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
        "print(\"Importance:\", feature_importance(rfc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRtGOs164q6C"
      },
      "source": [
        "### Задание 4 (1 балл)\n",
        "Теперь поработаем с реальными данными.\n",
        "\n",
        "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
        "\\\n",
        "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
        "\\\n",
        "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HruobK-q4q6C"
      },
      "outputs": [],
      "source": [
        "def read_dataset(path):\n",
        "    dataframe = pandas.read_csv(path, header=0)\n",
        "    dataset = dataframe.values.tolist()\n",
        "    random.shuffle(dataset)\n",
        "    y_age = [row[0] for row in dataset]\n",
        "    y_sex = [row[1] for row in dataset]\n",
        "    X = [row[2:] for row in dataset]\n",
        "\n",
        "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K0QXWr3b4q6C"
      },
      "outputs": [],
      "source": [
        "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
        "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9, random_state=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_best_params(X_train, y_train):\n",
        "    rfc = RandomForestClassifier()\n",
        "    param_grid = {\n",
        "        'n_estimators': [10, 20, 30],\n",
        "        'max_depth': [5, 10, 20],\n",
        "        'min_samples_leaf': [1, 3, 5],\n",
        "        'max_features': [3, 10,'auto'],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=rfc,\n",
        "        param_grid=param_grid,  \n",
        "        scoring='accuracy',\n",
        "        cv=3,\n",
        "        n_jobs=-1,\n",
        "        verbose=1 \n",
        "    )\n",
        "\n",
        "    grid.fit(X_train, y_train)\n",
        "    print(\"Лучшие параметры:\", grid.best_params_)\n",
        "    print(\"Accuracy:\", grid.best_score_)\n",
        "\n",
        "    return grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
            "Лучшие параметры: {'criterion': 'gini', 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 30}\n",
            "Accuracy: 0.698920207544524\n"
          ]
        }
      ],
      "source": [
        "best_params = find_best_params(X_train, y_age_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0y8J97m4q6C"
      },
      "source": [
        "#### Возраст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MLJykJZH4q6C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7011349306431274\n",
            "Most important features:\n",
            "1. mudakoff\n",
            "2. rhymes\n",
            "3. ovsyanochan\n",
            "4. styd.pozor\n",
            "5. 4ch\n",
            "6. dayvinchik\n",
            "7. rapnewrap\n",
            "8. pravdashowtop\n",
            "9. iwantyou\n",
            "10. tumblr_vacuum\n",
            "11. pixel_stickers\n",
            "12. reflexia_our_feelings\n",
            "13. bot_maxim\n",
            "14. memeboizz\n",
            "15. leprum\n",
            "16. ne1party\n",
            "17. pozor\n",
            "18. ultrapir\n",
            "19. xfilm\n",
            "20. thesmolny\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier(**best_params, random_state=21)\n",
        "\n",
        "rfc.fit(X_train, y_age_train)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
        "    print(str(i + 1) + \".\", name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
            "Лучшие параметры: {'criterion': 'gini', 'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'n_estimators': 20}\n",
            "Accuracy: 0.8437806759220305\n"
          ]
        }
      ],
      "source": [
        "best_params = find_best_params(X_train, y_sex_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgNpaAKH4q6D"
      },
      "source": [
        "#### Пол"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X-zne5-R4q6D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8461538461538461\n",
            "Most important features:\n",
            "1. 40kg\n",
            "2. mudakoff\n",
            "3. girlmeme\n",
            "4. modnailru\n",
            "5. 4ch\n",
            "6. 9o_6o_9o\n",
            "7. zerofat\n",
            "8. be.women\n",
            "9. be.beauty\n",
            "10. femalemem\n",
            "11. cook_good\n",
            "12. bon\n",
            "13. i_d_t\n",
            "14. rapnewrap\n",
            "15. recipes40kg\n",
            "16. woman.blog\n",
            "17. reflexia_our_feelings\n",
            "18. sh.cook\n",
            "19. beauty\n",
            "20. thesmolny\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier(**best_params, random_state=21)\n",
        "rfc.fit(X_train, y_sex_train)\n",
        "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
        "    print(str(i + 1) + \".\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxeTQylQ4q6D"
      },
      "source": [
        "### CatBoost\n",
        "В качестве аьтернативы попробуем CatBoost.\n",
        "\n",
        "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
        "\\\n",
        "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DOqVkEnd4q6D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Importance: [3.71769128e-03 4.63054971e-03 2.79505147e+01 2.79623985e+01\n",
            " 4.40755371e+01 3.20145165e-03]\n"
          ]
        }
      ],
      "source": [
        "X, y = synthetic_dataset(1000)\n",
        "\n",
        "cb_model = CatBoostClassifier(\n",
        "    iterations=100,           \n",
        "    depth=7,    \n",
        "    learning_rate=0.05,        \n",
        "    loss_function='MultiClass',\n",
        "    verbose=False,\n",
        "    random_state=21,\n",
        ")\n",
        "cb_model.fit(X, y)\n",
        "y_pred = cb_model.predict(X)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_pred, y))\n",
        "print(\"Importance:\", cb_model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcLRsSNG4q6E"
      },
      "source": [
        "### Задание 5 (3 балла)\n",
        "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
        "\\\n",
        "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hJGrQcO-4q6E"
      },
      "outputs": [],
      "source": [
        "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
        "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
        "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(\n",
        "    X_train, y_age_train, y_sex_train, train_size=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8dialjgMCu7_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "max_depth = range(1, 10, 3)\n",
        "min_samples_leaf = range(1, 10, 3)\n",
        "learning_rate = np.linspace(0.001, 1.0, 5)\n",
        "\n",
        "\n",
        "def get_best_params(y_train, y_eval):\n",
        "    best_score = 0\n",
        "    best_params = None\n",
        "    \n",
        "    max_depth = [4, 6, 8] \n",
        "    min_samples_leaf = [1, 3, 5] \n",
        "    learning_rate = [0.01, 0.1, 0.3] \n",
        "    \n",
        "    is_multiclass = len(np.unique(y_train)) > 2\n",
        "    loss_function = 'MultiClass' if is_multiclass else 'Logloss'\n",
        "    \n",
        "    for lr in learning_rate:\n",
        "        for depth in max_depth:\n",
        "            for min_samples in min_samples_leaf:\n",
        "                model = CatBoostClassifier(\n",
        "                    iterations=100,\n",
        "                    depth=depth,\n",
        "                    learning_rate=lr,\n",
        "                    min_data_in_leaf=min_samples,\n",
        "                    loss_function=loss_function,\n",
        "                    verbose=False,\n",
        "                    random_state=21\n",
        "                )\n",
        "                \n",
        "                model.fit(X_train, y_train, eval_set=(X_eval, y_eval), verbose=False)\n",
        "                score = model.score(X_eval, y_eval)\n",
        "                \n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {\n",
        "                        'learning_rate': lr,\n",
        "                        'depth': depth, \n",
        "                        'min_data_in_leaf': min_samples\n",
        "                    }\n",
        "                print(f\"lr={lr}, depth={depth}, leaf={min_samples}, score={score:.4f}\")\n",
        "                        \n",
        "    \n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5f_8eC4q6E"
      },
      "source": [
        "#### Возраст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yRqw46wuCu7_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr=0.01, depth=4, leaf=1, score=0.6300\n",
            "lr=0.01, depth=4, leaf=3, score=0.6300\n",
            "lr=0.01, depth=4, leaf=5, score=0.6300\n",
            "lr=0.01, depth=6, leaf=1, score=0.6552\n",
            "lr=0.01, depth=6, leaf=3, score=0.6552\n",
            "lr=0.01, depth=6, leaf=5, score=0.6552\n",
            "lr=0.01, depth=8, leaf=1, score=0.6678\n",
            "lr=0.01, depth=8, leaf=3, score=0.6678\n",
            "lr=0.01, depth=8, leaf=5, score=0.6678\n",
            "lr=0.1, depth=4, leaf=1, score=0.7064\n",
            "lr=0.1, depth=4, leaf=3, score=0.7064\n",
            "lr=0.1, depth=4, leaf=5, score=0.7064\n",
            "lr=0.1, depth=6, leaf=1, score=0.7141\n",
            "lr=0.1, depth=6, leaf=3, score=0.7141\n",
            "lr=0.1, depth=6, leaf=5, score=0.7141\n",
            "lr=0.1, depth=8, leaf=1, score=0.7302\n",
            "lr=0.1, depth=8, leaf=3, score=0.7302\n",
            "lr=0.1, depth=8, leaf=5, score=0.7302\n",
            "lr=0.3, depth=4, leaf=1, score=0.7127\n",
            "lr=0.3, depth=4, leaf=3, score=0.7127\n",
            "lr=0.3, depth=4, leaf=5, score=0.7127\n",
            "lr=0.3, depth=6, leaf=1, score=0.7330\n",
            "lr=0.3, depth=6, leaf=3, score=0.7330\n",
            "lr=0.3, depth=6, leaf=5, score=0.7330\n",
            "lr=0.3, depth=8, leaf=1, score=0.7246\n",
            "lr=0.3, depth=8, leaf=3, score=0.7246\n",
            "lr=0.3, depth=8, leaf=5, score=0.7246\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'learning_rate': 0.3, 'depth': 6, 'min_data_in_leaf': 1},\n",
              " np.float64(0.7330063069376314))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params, best_score = get_best_params(y_age_train, y_age_eval)\n",
        "best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qSeUpxPj4q6E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.712484237074401\n",
            "Most important features:\n",
            "1. ovsyanochan\n",
            "2. mudakoff\n",
            "3. styd.pozor\n",
            "4. rhymes\n",
            "5. dayvinchik\n",
            "6. leprum\n",
            "7. 4ch\n",
            "8. xfilm\n",
            "9. bot_maxim\n",
            "10. rapnewrap\n"
          ]
        }
      ],
      "source": [
        "cb_model = CatBoostClassifier(iterations=100,                 \n",
        "    loss_function='MultiClass',\n",
        "    verbose=False,\n",
        "    random_state=21,\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "cb_model.fit(X_train, y_age_train)\n",
        "y_pred = cb_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_age_test, y_pred))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(cb_model.feature_importances_, features, 10)):\n",
        "    print(str(i + 1) + \".\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfYSptm74q6E"
      },
      "source": [
        "#### Пол"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xSgjSSh4Cu7_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr=0.01, depth=4, leaf=1, score=0.8059\n",
            "lr=0.01, depth=4, leaf=3, score=0.8059\n",
            "lr=0.01, depth=4, leaf=5, score=0.8059\n",
            "lr=0.01, depth=6, leaf=1, score=0.8374\n",
            "lr=0.01, depth=6, leaf=3, score=0.8374\n",
            "lr=0.01, depth=6, leaf=5, score=0.8374\n",
            "lr=0.01, depth=8, leaf=1, score=0.8472\n",
            "lr=0.01, depth=8, leaf=3, score=0.8472\n",
            "lr=0.01, depth=8, leaf=5, score=0.8472\n",
            "lr=0.1, depth=4, leaf=1, score=0.8662\n",
            "lr=0.1, depth=4, leaf=3, score=0.8662\n",
            "lr=0.1, depth=4, leaf=5, score=0.8662\n",
            "lr=0.1, depth=6, leaf=1, score=0.8753\n",
            "lr=0.1, depth=6, leaf=3, score=0.8753\n",
            "lr=0.1, depth=6, leaf=5, score=0.8753\n",
            "lr=0.1, depth=8, leaf=1, score=0.8767\n",
            "lr=0.1, depth=8, leaf=3, score=0.8767\n",
            "lr=0.1, depth=8, leaf=5, score=0.8767\n",
            "lr=0.3, depth=4, leaf=1, score=0.8774\n",
            "lr=0.3, depth=4, leaf=3, score=0.8774\n",
            "lr=0.3, depth=4, leaf=5, score=0.8774\n",
            "lr=0.3, depth=6, leaf=1, score=0.8739\n",
            "lr=0.3, depth=6, leaf=3, score=0.8739\n",
            "lr=0.3, depth=6, leaf=5, score=0.8739\n",
            "lr=0.3, depth=8, leaf=1, score=0.8711\n",
            "lr=0.3, depth=8, leaf=3, score=0.8711\n",
            "lr=0.3, depth=8, leaf=5, score=0.8711\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'learning_rate': 0.3, 'depth': 4, 'min_data_in_leaf': 1},\n",
              " np.float64(0.877365101611773))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params, best_score = get_best_params(y_sex_train, y_sex_eval)\n",
        "best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4rKa-f6F4q6E",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8675914249684742\n",
            "Most important features:\n",
            "1. 40kg\n",
            "2. girlmeme\n",
            "3. modnailru\n",
            "4. mudakoff\n",
            "5. academyofman\n",
            "6. i_d_t\n",
            "7. thesmolny\n",
            "8. femalemem\n",
            "9. igm\n",
            "10. zerofat\n"
          ]
        }
      ],
      "source": [
        "cb_model = CatBoostClassifier(iterations=100,                 \n",
        "    loss_function='MultiClass',\n",
        "    verbose=False,\n",
        "    random_state=21,\n",
        "    **best_params\n",
        ")\n",
        "cb_model.fit(X_train, y_sex_train)\n",
        "y_pred = cb_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_sex_test, y_pred))\n",
        "print(\"Most important features:\")\n",
        "for i, name in enumerate(most_important_features(cb_model.feature_importances_, features, 10)):\n",
        "    print(str(i + 1) + \".\", name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
